{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35bb336-3d56-40c2-8676-55057c596917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoice Processing:\nStarting streaming query (trigger once)...\nStreaming load complete.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0xff74146228a0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import expr, explode, col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "class invoiceStream:\n",
    "\n",
    "    def __init__(self):\n",
    "        # INPUT JSON and OUTPUTS (adjust paths/table names as you like)\n",
    "        self.base_path = \"/Volumes/spark-catalog/sandbox/sample_data/invoices\"\n",
    "        self.checkpoint = \"/Volumes/spark-catalog/sandbox/sample_data/invoices_1/checkpoint\"\n",
    "        self.out_table = \"invoice_flattened\"\n",
    "\n",
    "    # DDL schema exactly as in your screenshot\n",
    "    def getSchema(self):\n",
    "        return \"\"\"\n",
    "        InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "        CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n",
    "        PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double,\n",
    "        DeliveryType string,\n",
    "        DeliveryAddress struct<AddressLine string, City string, ContactNumber string, PinCode string,\n",
    "        State string>,\n",
    "        InvoiceLineItems array<struct<ItemCode string, ItemDescription string,\n",
    "        ItemPrice double, ItemQty bigint, TotalValue double>>\n",
    "        \"\"\"\n",
    "\n",
    "    # STREAM reader (tries first)\n",
    "    def read_invoices_stream(self):\n",
    "        return (\n",
    "            spark.readStream\n",
    "                .format(\"json\")\n",
    "                .schema(self.getSchema())\n",
    "                .load(self.base_path)\n",
    "        )\n",
    "\n",
    "    # BATCH reader (fallback if streaming not supported)\n",
    "    def read_invoices_batch(self):\n",
    "        return (\n",
    "            spark.read\n",
    "                .format(\"json\")\n",
    "                .schema(self.getSchema())\n",
    "                .load(self.base_path)\n",
    "        )\n",
    "\n",
    "    # explode array and keep useful top-level fields\n",
    "    def explode_invoices(self, invoice_df):\n",
    "        return (\n",
    "            invoice_df.selectExpr(\n",
    "                \"InvoiceNumber\", \"CreatedTime\", \"StoreID\", \"PosID\",\n",
    "                \"CashierID\", \"CustomerType\", \"PaymentMethod\", \"DeliveryType\",\n",
    "                \"DeliveryAddress.City as City\", \"DeliveryAddress.State as State\",\n",
    "                \"explode(InvoiceLineItems) as lineItem\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # project fields from each line item\n",
    "    def transform_invoices(self, exploded_df):\n",
    "        return (\n",
    "            exploded_df.select(\n",
    "                \"InvoiceNumber\", \"CreatedTime\", \"StoreID\", \"PosID\",\n",
    "                \"CashierID\", \"CustomerType\", \"PaymentMethod\", \"DeliveryType\",\n",
    "                \"City\", \"State\",\n",
    "                expr(\"lineItem.ItemCode\").alias(\"ItemCode\"),\n",
    "                expr(\"lineItem.ItemDescription\").alias(\"ItemDescription\"),\n",
    "                expr(\"lineItem.ItemPrice\").alias(\"ItemPrice\"),\n",
    "                expr(\"lineItem.ItemQty\").alias(\"ItemQty\"),\n",
    "                expr(\"lineItem.TotalValue\").alias(\"TotalValue\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # STREAM writer\n",
    "    def write_invoices_stream(self, transformed_df):\n",
    "        return (\n",
    "            transformed_df.writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", self.checkpoint)\n",
    "                .outputMode(\"append\")\n",
    "                .trigger(once=True)\n",
    "                .toTable(self.out_table)\n",
    "        )\n",
    "\n",
    "    # BATCH writer (fallback)\n",
    "    def write_invoices_batch(self, transformed_df):\n",
    "        (\n",
    "            transformed_df.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .saveAsTable(self.out_table)\n",
    "        )\n",
    "\n",
    "    def process_invoices(self):\n",
    "        print(\"Invoice Processing:\")\n",
    "        try:\n",
    "            # Try streaming path\n",
    "            invoice_df = self.read_invoices_stream()\n",
    "            exploded_df = self.explode_invoices(invoice_df)\n",
    "            transformed_df = self.transform_invoices(exploded_df)\n",
    "            print(\"Starting streaming query (trigger once)...\")\n",
    "            q = self.write_invoices_stream(transformed_df)\n",
    "            q.awaitTermination()\n",
    "            print(\"Streaming load complete.\")\n",
    "            return q\n",
    "        except AnalysisException as e:\n",
    "            # Fallback to batch if streaming not supported in your workspace\n",
    "            print(\"Streaming not available; falling back to batch. Reason:\", str(e))\n",
    "            invoice_df = self.read_invoices_batch()\n",
    "            exploded_df = self.explode_invoices(invoice_df)\n",
    "            transformed_df = self.transform_invoices(exploded_df)\n",
    "            print(\"Transformed Invoices\",transformed_df)\n",
    "            self.write_invoices_batch(transformed_df)\n",
    "            print(\"Batch load complete.\")\n",
    "            return None\n",
    "\n",
    "# Run it\n",
    "invoiceStream().process_invoices()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "invoice_data_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}